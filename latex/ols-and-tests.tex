\section{Metoda najmniejszych kwadratów (MNK)}\label{sec:mnk}

\subsection{Opis}\label{subsec:opis}
Metoda najmniejszych kwadratów służy do estymacji parametrów w modelu ekonometrycznym.

\[ \alpha = (X^{T}X)^{-1}X^{T}y \]

\subsection{Założenia MNK}\label{subsec:założenia-mnk}

\begin{itemize}
    \item Zmienne objaśniające są nielosowe i nieskorelowane ze składnikiem losowym
    \item \[E(\varepsilon) = 0\]
    \item \[Var(\varepsilon) = \sigma^2 \iota\]
    \item \[rank(X) = (k+1) <n\]
\end{itemize}

\subsection{Twierdzenie Gaussa-Markowa}\label{subsec:twierdzenie-gaussa-markowa}
Przy spełnieniu założeń MNK estymator jest BLUE
\begin{itemize}
    \item \textbf{B}est
    \item \textbf{L}inear
    \item \textbf{U}nbiased
    \item \textbf{E}stimator
\end{itemize}

\section {Poprawność statystyczna modelu}\label{sec:poprawność-statystyczna-modelu}

\subsection{Występowanie efektu katalizy}\label{subsec:występowanie-efektu-katalizy}

Mówimy, że w modelu ekonometrycznym określonym przez regularną
parę korelacyjną (R, \(R_0\)) występuje efekt katalizy, jeżeli istnieje taka
para wskaźników (i, j), dla której

\begin{equation}
    r_{ij} < 0 \; lub \; r_{ij}  > \frac{r_i}{r_j}
\end{equation}

Nateżenie efektu katalizy mierzymy wzorem

\[ \eta = R^2 - H \]

Gdzie H oznacza integralną pojemność informacyjną zmiennych objaśniających.

\subsection{Koincydentność}\label{subsec:koincydentność}

Mowimy, że zmienna jest koincydentna, jeżeli:

\begin{equation}
    sgn(r_1) = sign(a_i)
\end{equation}

Koincydencja jest zjawiskiem porządanym w modelu.

\subsection{Istotność współczynnika determinacji}\label{subsec:istotność-współczynnika-determinacji}

Test ten pozwala sprawdzić istotność wszystkich zmiennych w modelu naraz.

\begin{equation}
    \begin{split}
        &H_0: \alpha_1 = \alpha_2 = \dots = \alpha_k = 0 \\
        &H_1: \alpha_1 \ne 0 \; lub \; \alpha_2 \ne 0 \; lub \; \dots \; lub \; \alpha_k \ne 0
    \end{split}
\end{equation}

Statystyka F-Snedachora ma postać \(F = \frac{R^2}{k} \frac{n-k-1}{1-R^2}\) o \(r_1 = k\) i \(r_2 = n-k-1\) stopniach swobody.
Jeżeli \(F > F_\alpha,r1,r2 \) to odrzucamy \(H_0\).
Oznacza to, że przynajmniej jedna zmienna objaśniająca jest statystycznie ważna w modelu.

\subsection{Normalność reszt}\label{subsec:normalność-reszt}

Jednym z założeń MNK jest normalność reszt.

\subsubsection{Test Jarque-Bera}
Test Jarque-Bera polega na obliczeniu skośności oraz kurtozy w modelu i porównanie ich do znanym wartości z rozkładu normalnego

\begin{equation}
    \begin{split}
        &H_0: \varepsilon \sim N(0,\sigma) \\
        &H_1: \varepsilon \not\sim N(0,\sigma)
    \end{split}
\end{equation}

Statystyke testową wyznaczamy ze wzoru:

\begin{equation}
    JB = n (\frac{1}{6}B_1 + \frac{1}{24}(B_2 -3)^2)
\end{equation}

Gdzie:

\begin{equation}
    \begin{split}
        &\bar{S} = \sqrt{\frac{1}{n}\sum_{t=1}^{n}e^2_t} \\
        &\sqrt{B_1} = \frac{1}{n\bar{S^3}\sum_{t=1}^{n}e^3_t}  \\
        & B_2 = \frac{1}{n\bar{S^4}\sum_{t=1}^{n}{e^4_t}}
    \end{split}
\end{equation}

Statystyka JB ma rozklad \(\chi^2\) z dwoma stopniami swobody.

Jeżeli \(JB > \chi^2_2\) to odrzucamy \(H_0\).

\subsubsection{Centralne twierdzenie graniczne}

Jeżeli próbę z populacji o sredniej \(\mu\) i skończinym odchyleniu standardowym \(\sigma \), to rozkład sredniej z próby, \(\bar{X}\), daży do rozkładu normalnego o sredniej \(\mu\) i odchyleniu standardowym \(\sqrt{\sigma} \), gdy liczebnosć próby wzrasta nieograniczenie, czyli dla "dostatecznie dużych" n.

\begin{equation}
    \begin{split}
        \frac{\frac{1}{n} \sum_{i=1}^{n} X_i - \mu}{\frac{\sigma}{\sqrt{n}}}
    \end{split}
\end{equation}

W moim modelu występuje około 20 tysięcy obserwacji, więc przy sprawdzaniu normalności rozkładu można powołać się na to twierdzenie.

\subsection{Istotność statystyczna pojedyńczych zmiennych objaśniających}\label{subsec:istotność-statystyczna-zmiennych-objaśniających}
Jeżeli spełnione jest założenie o normalności reszt, to możemy postawić następujące hipotezę

\begin{equation}
    \begin{split}
        &H_0: \alpha_j = 0 \\
        &H_1: \alpha_j \ne 0
    \end{split}
\end{equation}

Statystyka t ma postać \(t = \frac{\alpha j}{S_{\alpha j}}\).
Jeżeli \(|t| > t_{n -(k+1)}\) to odrzucamy \(H_0\).
Oznacza to brak wplywu zmiennej \(X_j\) na Y.

\subsection{Liniowość modelu - test liczby serii}\label{subsec:liniowość-modelu---test-liczby-serii}

\begin{equation}
    \begin{split}
        &H_0: \text{postać modelu jest liniowa} \\
        &H_1: \text{postać modelu nie jest liniowa}
    \end{split}
\end{equation}

Reszty modelu należy ułożyć wzgledem posortowanej rosnąco zmiennej objaśnianej, a następnie policzyć dla nich ilość serii.
Pod pojęciem serii rozumiemy każdy ciąg elementów o identycznych znakach.

\begin{equation}
    \begin{split}
        &\mu = \frac{2N_+ N_-}{N} +1 \\
        &\sigma^2 = \frac{(\mu -1)(\mu-2)}{N-1} \\
        &Z = \frac{R - \mu}{\sigma}
    \end{split}
\end{equation}


Gdzie

\begin{equation}
    \begin{split}
        &N_+ - \text{ilość dodatnich elementów} \\
        &N_- - \text{ilość ujemnych elementów} \\
        &N - text{ilość elementów} \\
        &R - text{ilość serii} \\
    \end{split}
\end{equation}


Jeżeli \(|Z| > Z_{1-\alpha/2}\) to odrzucamy \(H_0\).

\subsection{Stabilność parametrów modelu - test Chowa}\label{subsec:stabilność-parametrów-modelu}

Do sprawdzenia stabilności parametrów modelu można wykorzystać test Chowa.

\begin{equation}
    \begin{split}
        &H_0: parametry \;  modelu \; są \; stabilne \\
        &H_1: parametry \;  modelu \; nie \; są \; stabilne
    \end{split}
\end{equation}

Obserwacje wykorzystane przy wyznaczaniu MNK dzielimy na 2 podgrupy.

\begin{equation}
    \begin{split}
        &y_t = \beta_0 + \beta_{1}x_{1t} + \dots + \beta_{k}x_{kt} + \varepsilon_{1t}, \;  t= 1,2, \dots , n_1  \\
        &y_t = \gamma_0 + \gamma_{1}x_{1t} + \dots + \gamma_{k}x_{kt} + \varepsilon_{1t}, \;  t= n_1,n_1 +1, \dots , n
    \end{split}
\end{equation}

Następnie wyznaczamy statystyke F.

\begin{equation}
    \begin{split}
        F = \frac{RSK - (RSK_I - RSK_{II})}{(RSK_{II}  + RSK_{II})} \frac{n-2(k+1)}{n+1}
    \end{split}
\end{equation}

Gdzie

\begin{equation}
    \begin{split}
        &RSK = \sum_{t=1}^{n} e_{t}^{2} \\
        &RSK_{I} = \sum_{t=1}^{n_1} e_{t}^{2} \\
        &RSK_{II} = \sum_{t=n_1}^{n} e_{t}^{2}
    \end{split}
\end{equation}

Statystyka F ma rozkład F-Snedachora z \(r_1=k+1 \) i \(r_2= n -2(k+1) \) stopniami swobody.
Jeżeli \( F > F_{\alpha,r_1,r_2} \) to odrzucamy \(H_{0}\).

\subsection{Współliniowość zmiennych objaśniających}\label{subsec:współliniowość-zmiennych-objaśniających}

Z założeń MNK wynika, że rzad macierzy X jest równy liczbie kolumn tej macierzy \((k+1)\).
Oznacza to, że kolumny macierzy są liniowo niezależne.

Aby sprawdzić współliniowość zmiennych w modelu

\begin{equation}
    y = \alpha_{0} + \alpha_{1}X_{1} + \dots + \alpha{k}X_{k} + \varepsilon
\end{equation}

Szacujemy pomocnicze modele, w których zmienna objaśnianą jest \(X_i\) , a zmiennymi objaśniającymi pozostałe zmienne oraz wyraz wolny.
Dla modeli \(i=1,2, \dots ,k \) wyznaczamy odpowiednie współczynniki determinacji.
Jeżeli \(R_{i}^{2} > 0.9\) to w modelu zachodzi zjawisko współliniowości, które zakłóca jakość modelu.
Należy wtedy usunąć tą zmienna z modelu, jednakże może on się wtedy okazać bezużyteczny.

\subsection{Autokorelacja składnika losowego - test Breuscha-Godfreya}\label{subsec:autokorelacja-składnika-losowego}

\begin{equation}
    \begin{split}
        &H_0: \rho = 0 \\
        &H_1: \rho > 0
    \end{split}
\end{equation}

Szacujemy model pomocniczy

\begin{equation}
    e_{t} = \beta_{0} + \beta_{1}x_{1t} + \beta_{2}x_{2t} + \dots + \beta_{k}x_{kt}\varepsilon_{t-1} + \mu_{t}
\end{equation}

Nastepnie obliczmy statystyke testową

\begin{equation}
    LM = (n-1)R^2
\end{equation}

Dla dużej liczby obserwacji \((n > 30)\) statystyka testowa ma rozkład \(\chi^2\) z jednym stopniem swobody.

\subsection{Heteroskedastyczność składnika losowego - test Breuscha-Pagana}\label{subsec:heteroskedastyczność-składnika-losowego}

Występowanie heteroskedastycznośc w modelu sprawia, że składniki losowe mimo braku wzjemnej koleracji maja rózne wariacje, co jest niezgodne z założeniami MNK.
Estymator parametrów wyestymowany w sytuacji wystepowania heteroskedastyczności nie jest najefektywniejszy.


\begin{equation}
    \begin{split}
        &H_0: \test{reszty są homoskedastyczne}\\
        &H_1: \text{reszty sa heteroschedastyczne}
    \end{split}
\end{equation}

Dla wyznaczonych reszt modelu przeprowadzamy regresje pomocnicza

\begin{equation}
    \begin{split}
        \frac{\epsilon_{i}^{2}}{\sigma^2} = \gamma_{0} + \gamma_{1}z_{i} + \mu_{i}
    \end{split}
\end{equation}

Statystyka testowa
\begin{equation}
    \begin{split}
        LM = (n-1)R^{2}
    \end{split}
\end{equation}

Dla dużej liczby obserwacji \((n > 30)\) statystyka testowa ma rozkład \(\chi^{2}\) z \(k\) stopniami swobody.